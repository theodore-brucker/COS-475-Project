{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder with CNN\n",
    "\n",
    "475 Final Project that creates an Autoencoder with CNN layers  \n",
    "Trains on flows from CICIDS2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset\n",
    "dataset_path = 'C:/Users/theob/Code/COS-475-Project/Dataset/Clean CSE/Cleaned Wed and Fri.csv'\n",
    "flows = pd.read_csv(dataset_path)\n",
    "flows_small = flows[0:30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the flow dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    initial_columns_count = df.shape[1]\n",
    "    initial_rows_count = df.shape[0]\n",
    "\n",
    "    df = df.copy()  # Work on a copy to avoid side-effects on the original DataFrame\n",
    "\n",
    "    # Convert all object columns to numeric, errors turn into NaNs\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Replace infinities with NaNs for mean calculation\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Fill NaN values with the mean of the columns, warn if the column is all NaN\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().all():\n",
    "            print(f\"Warning: Column {col} is entirely NaN.\")\n",
    "        mean_value = df[col].mean()\n",
    "        if pd.isna(mean_value):\n",
    "            print(f\"Warning: Mean is NaN for column {col}.\")\n",
    "        df[col].fillna(mean_value, inplace=True)\n",
    "\n",
    "    # Remove columns without any numeric data or that remain all NaN\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # Replace negative values with zero or mean, based on requirement\n",
    "    df[df < 0] = 0\n",
    "\n",
    "    # Drop all columns that contain only zeros\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_columns_count = df.shape[1]\n",
    "    final_rows_count = df.shape[0]\n",
    "\n",
    "    print(f\"Initial number of columns: {initial_columns_count}, final number of columns: {final_columns_count}\")\n",
    "    print(f\"Initial number of rows: {initial_rows_count}, final number of rows: {final_rows_count}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_labels(labels):\n",
    "    # Converts a list of labels into integer format where each unique label is assigned a unique integer.\n",
    "    # Additionally, prints the total number of unique labels and the number of occurrences of each label.\n",
    "\n",
    "    # Create a dictionary to map each label to a unique integer\n",
    "    unique_labels = sorted(set(labels))\n",
    "    label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Count occurrences of each label\n",
    "    label_counts = collections.Counter(labels)\n",
    "    \n",
    "    # Print the total number of unique labels and occurrences of each\n",
    "    print(f\"Total number of unique labels: {len(unique_labels)}\")\n",
    "    print(\"Occurrences of each label:\")\n",
    "    for label, count in sorted(label_counts.items(), key=lambda x: label_mapping[x[0]]):\n",
    "        print(f\"{label}: {count}\")\n",
    "\n",
    "    # Apply the mapping to the labels list to create the encoded labels list\n",
    "    encoded_labels = [label_mapping[label] for label in labels]\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "# Encode and separate the labels from the features\n",
    "flows_small['Label'] = encode_labels(flows_small['Label'])\n",
    "\n",
    "# Clean the feature space and drop 5 columns to create a 64 feature shape\n",
    "flows_semi_reduced = clean_dataframe(flows_small.drop(['Timestamp', 'Active Max',\n",
    "       'Active Min',], axis=1))\n",
    "\n",
    "flow_labels = flows_semi_reduced['Label']\n",
    "flows_reduced = flows_semi_reduced.drop(['Label'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create indices for mapping back to original inputs later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = flows_reduced.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D-ify with Triangle Area Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matrix(vector):\n",
    "    vector_transformed = np.square(vector)\n",
    "    matrix = np.outer(vector_transformed, vector_transformed)\n",
    "    max_val = np.max(matrix)\n",
    "    if max_val > 0:\n",
    "        matrix /= max_val\n",
    "    return matrix\n",
    "\n",
    "def vectors_to_matrices(df):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "    \n",
    "    # Ensure all data is numeric\n",
    "    if not np.all(df.map(np.isreal).all()):\n",
    "        raise ValueError(\"All columns must be numeric\")\n",
    "\n",
    "    # Convert all columns to float for consistent processing and handling\n",
    "    df = df.astype(float)\n",
    "\n",
    "    # Use vectorized operations where possible\n",
    "    matrices = [compute_matrix(df.iloc[i].values) for i in range(len(df))]\n",
    "\n",
    "    return matrices\n",
    "\n",
    "def tam(df):\n",
    "    matrices = []\n",
    "    for index, vector in df.iterrows():  # Iterating correctly over rows\n",
    "        if pd.api.types.is_numeric_dtype(vector):  # Ensuring vector contains numeric data\n",
    "            # Convert the Series to a proper numeric type if not already\n",
    "            vector = vector.astype(float)\n",
    "            # Calculate the outer product of the vector with itself\n",
    "            matrix = np.outer(vector.values.T, vector.values.T)\n",
    "            matrices.append(matrix)\n",
    "    return matrices\n",
    "\n",
    "matrices = compute_matrix(flows_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert matrices into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_tensors = [torch.tensor(matrix, dtype=torch.float32) for matrix in matrices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split with the indices mapping to original inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Step 1: Separate benign and malicious data along with their indices\n",
    "benign_indices = [idx for idx, label in enumerate(flow_labels) if label == 0]\n",
    "malicious_indices = [idx for idx, label in enumerate(flow_labels) if label != 0]\n",
    "\n",
    "benign_matrices = [matrices[idx] for idx in benign_indices]\n",
    "malicious_matrices = [matrices[idx] for idx in malicious_indices]\n",
    "\n",
    "benign_labels = [0] * len(benign_indices)\n",
    "malicious_labels = [flow_labels[idx] for idx in malicious_indices]\n",
    "\n",
    "# Step 2: Use all benign data for training\n",
    "X_train = benign_matrices\n",
    "y_train = benign_labels\n",
    "idx_train = list(range(len(benign_matrices)))  # Resetting indices for training\n",
    "\n",
    "# Step 3: Test set includes all malicious and an equal amount of benign\n",
    "X_test = malicious_matrices + benign_matrices[:len(malicious_matrices)]  # Equal count for both classes\n",
    "y_test = malicious_labels + benign_labels[:len(malicious_matrices)]\n",
    "idx_test = list(range(len(X_test)))  # Resetting indices for test\n",
    "\n",
    "# Shuffle test data to ensure random distribution\n",
    "X_test, y_test, idx_test = shuffle(X_test, y_test, idx_test, random_state=42)\n",
    "\n",
    "# Output sizes and distributions\n",
    "print(\"Training set size:\", len(X_train), \"Test set size:\", len(X_test))\n",
    "print(\"Train indices:\", idx_train, \"Test indices:\", idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, indices, labels=None, augment=False, noise_level=0.01):\n",
    "        self.data = data\n",
    "        self.indices = indices\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            actual_idx = self.indices[idx]\n",
    "            data_item = torch.tensor(self.data[actual_idx], dtype=torch.float32).unsqueeze(0)\n",
    "            label = torch.tensor(self.labels[actual_idx], dtype=torch.long) if self.labels else torch.tensor(-1)\n",
    "            if torch.isnan(data_item).any() or torch.isinf(data_item).any():\n",
    "                print(f\"NaN or Inf found in batch at index {idx}\")\n",
    "            return data_item, label, actual_idx\n",
    "        except IndexError as e:\n",
    "            print(f\"IndexError: {e} - Requested idx: {idx}, Actual idx: {actual_idx}, Available data length: {len(self.data)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e} - Accessing index: {idx}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the dataset and dataloader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(X_train, idx_train, y_train, augment=True)\n",
    "test_dataset = CustomDataset(X_test, idx_test, y_test, augment=False)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate that the data loader objects arent garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataloader(dl, name=\"DataLoader\"):\n",
    "    try:\n",
    "        data_iter = iter(dl)\n",
    "        first_batch = next(data_iter)\n",
    "        print(f\"{name} - Batch Content: {first_batch}\")  # Debugging line to see the batch content\n",
    "\n",
    "        if len(first_batch) != 3:\n",
    "            print(f\"Error: Batch does not contain three elements. Contains: {len(first_batch)} elements\")\n",
    "            return\n",
    "\n",
    "        flows, labels, indices = first_batch\n",
    "        print(f\"{name} - First batch shapes: Features: {flows.shape}, Labels: {labels.shape}, Indices: {indices}\")\n",
    "        print(f\"{name} - Data types: Features: {flows.dtype}, Labels: {labels.dtype}\")\n",
    "        print(f\"{name} - Sample labels: {labels[:10]}\")\n",
    "        print(f\"{name} - Sample indices: {indices[:10]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option to save the dataloader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_loader, \"C:/Users/theob/Code/COS-475-Project/Data-for-wilder/CIC-train-clean.pth\")\n",
    "torch.save(test_loader, \"C:/Users/theob/Code/COS-475-Project/Data-for-wilder/CIC-test-clean.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option to load in the prebuilt dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.load('C:/Users/theob/Code/COS-475-Project/Data-for-wilder/CIC-train-clean.pth')\n",
    "test_loader = torch.load('C:/Users/theob/Code/COS-475-Project/Data-for-wilder/CIC-test-clean.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the shape of the batches and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels, idx in train_loader:\n",
    "    input_width = data.shape[0]\n",
    "    print(\"Input batch shape from DataLoader:\", data.shape)\n",
    "    print(\"Labels batch shape from DataLoader:\", labels.shape)\n",
    "    print(\"Indices batch shape from DataLoader:\", idx.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure that cuda is available, else run on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import Dropout\n",
    "\n",
    "class ConvSequenceAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvSequenceAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=(5, 5), padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=(5, 5), padding=1),\n",
    "            BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            Dropout(0.5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)  # Compressing the input to a latent space representation\n",
    "        x = self.decoder(x)  # Attempting to reconstruct the original input from the latent representation\n",
    "        return x\n",
    "\n",
    "model = ConvSequenceAutoencoder().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test an input to ensure input and output are same shape for MSE calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_autoencoder():\n",
    "    input_tensor = torch.randn(64, 1, 61, 61) # Modify to be the shape of input check from earlier\n",
    "    print(\"Input shape:\", input_tensor.shape)\n",
    "    \n",
    "    # Forward the input tensor through the model\n",
    "    output_tensor = model(input_tensor)\n",
    "    print(\"Output shape:\", output_tensor.shape)\n",
    "\n",
    "# Call the test function to check input and output shapes\n",
    "test_autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epochs=5, patience=1, min_delta=0.001):\n",
    "    model.train()  # Set the model to training mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0  # Counter to keep track of the number of epochs with no improvement\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, _, _ in train_loader:  # Only taking inputs, no labels or indices\n",
    "            \n",
    "            # Send the inputs to device\n",
    "            inputs = inputs.to(device) \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss for current input\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate average loss for epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}: Loss = {avg_loss:.4f}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if best_loss - avg_loss > min_delta:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "            print(\"Loss improved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(\"No improvement in loss.\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "                break\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.00001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize reconstruction error on benign vs each type of attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(reconstructed, inputs):\n",
    "    return ((reconstructed - inputs) ** 2).mean(dim=[1, 2, 3])\n",
    "\n",
    "def accumulate_errors(model, test_loader, device):\n",
    "    reconstruction_errors = []\n",
    "    true_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            reconstructed = model(inputs)\n",
    "            mse = compute_mse(reconstructed, inputs)\n",
    "            reconstruction_errors.append(mse.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    reconstruction_errors = np.concatenate(reconstruction_errors)\n",
    "    return reconstruction_errors, true_labels\n",
    "\n",
    "def find_best_threshold(reconstruction_errors, true_labels, thresholds):\n",
    "    best_accuracy = 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        predictions = (reconstruction_errors > threshold).astype(int)\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_threshold = threshold\n",
    "        print(f'Threshold: {threshold:.4f}, Accuracy: {accuracy*100:.2f}%')\n",
    "\n",
    "    return best_threshold, best_accuracy\n",
    "\n",
    "def plot_all_classes(reconstruction_errors, true_labels, threshold):\n",
    "    error_dict = {}\n",
    "    unique_labels = np.unique(true_labels)\n",
    "    for label in unique_labels:\n",
    "        error_dict[label] = reconstruction_errors[true_labels == label]\n",
    "\n",
    "    plot_histogram(error_dict, threshold, unique_labels)\n",
    "\n",
    "def plot_histogram(error_dict, threshold, labels, zoom_percentile=100):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = plt.cm.get_cmap('viridis', len(labels))\n",
    "\n",
    "    # Clean data: remove any NaN values from error_dict\n",
    "    for key in error_dict.keys():\n",
    "        error_dict[key] = error_dict[key][np.isfinite(error_dict[key])]\n",
    "\n",
    "    all_errors = np.concatenate(list(error_dict.values()))\n",
    "    if all_errors.size == 0:\n",
    "        print(\"No valid error data available to plot.\")\n",
    "        return\n",
    "    \n",
    "    upper_limit = max(np.percentile(all_errors, zoom_percentile), threshold)\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        plt.hist(error_dict[label], bins=20, alpha=0.7, label=f'Label {label}', color=colors(idx))\n",
    "    \n",
    "    plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold = {threshold:.2f}')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reconstruction Errors by Class')\n",
    "    plt.xlim(left=0, right=upper_limit)\n",
    "    plt.show()\n",
    "\n",
    "# Main execution workflow\n",
    "errors, labels = accumulate_errors(model, test_loader, device)\n",
    "thresholds = np.linspace(0.001, 0.01, num=50)  # Example range and granularity\n",
    "best_threshold, best_accuracy = find_best_threshold(errors, labels, thresholds)\n",
    "print(f'Best Threshold: {best_threshold:.4f}, Highest Accuracy: {best_accuracy*100:.2f}%')\n",
    "plot_all_classes(errors, labels, best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the original inputs that were misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def map_predictions_to_original(test_loader, model, device, anomaly_threshold):\n",
    "    misclassified_malicious = []\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, indices in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate the reconstruction error\n",
    "            reconstruction_error = F.mse_loss(outputs, inputs, reduction='none')\n",
    "            reconstruction_error = reconstruction_error.mean([1, 2, 3])  # Average over all dimensions except the batch\n",
    "            \n",
    "            # Determine if the reconstruction error exceeds the threshold\n",
    "            anomalies = (reconstruction_error > anomaly_threshold).int()\n",
    "\n",
    "            # Calculate mismatches and filter for misclassified malicious entries\n",
    "            labels = labels.to(device)\n",
    "            mismatches = (anomalies != labels).cpu().numpy()\n",
    "            misclassified = indices.cpu().numpy()[mismatches & (labels.cpu().numpy() == 1)]  # Focus on false negatives\n",
    "            misclassified_malicious.extend(misclassified)\n",
    "    \n",
    "    # Save the misclassified malicious entries to a CSV file\n",
    "    misclassified_df = flows_reduced.loc[misclassified_malicious]\n",
    "    misclassified_df.to_csv('misclassified_malicious1.csv', index=False)\n",
    "    return misclassified_malicious\n",
    "\n",
    "# Example usage:\n",
    "anomaly_threshold = 0.0527  # Define a suitable threshold based on your application\n",
    "results = map_predictions_to_original(test_loader, model, device, anomaly_threshold)\n",
    "print(\"Misclassified malicious entries recorded.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
